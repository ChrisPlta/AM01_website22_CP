---
categories:  
- ""
- ""
date: "2022-09-11"
description: A topic discussed even more than whether Mark Zuckerberg is indeed a lizard, climate change poses a threat to the entire world. But apart from listening to millions of different media claims, let us look at the data ourselves! # the title that will show up once someone gets to this page
draft: false
image: climate_change.jpg # save picture in \static\img\blogs. Acceptable formats= jpg, jpeg, or png . Your iPhone pics wont work

keywords: ""
slug: climate_change # slug is the shorthand URL address... no spaces plz
title: Climate change - hoax or fact?
---

Climate change has been a topic that has been following me around for as long as I can remember! It is arguably one of the biggest crisis of our time and a major threat to human civilization. Again and again we see major headlines in the newspaper, ranging from the entertaining that cows are essentially driving us to extinction with their [burps](https://www.pbs.org/newshour/show/cow-burps-are-a-major-contributor-to-climate-change-can-scientists-change-that), to climate activist Greta Thunberg throwing shade at [world leaders](https://www.youtube.com/watch?v=TMrtLsQbaok). While there is some very educational content out there ([Kurzgesagt](https://www.youtube.com/watch?v=wbR-5mHI6bo) has some great content on it), I figured I should take a look at the situation myself. With data provided by NASA on climate of the Northern Hemisphere for the last several years, I looked at whether there indeed is a global warming happening.

```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

knitr::opts_chunk$set(
  fig.width=8, 
  fig.height=6,
  fig.align = "center"
)
```

```{r load-libraries, include=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(wbstats)
library(countrycode)
library(patchwork)
library(gganimate)
library(infer)
```

```{r weather_data, cache=TRUE}

weather <- 
  read_csv("https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv", 
           skip = 1, 
           na = "***")

```

```{r tidyweather}

# Select only year and month data from the weather data set
weather_clean <- weather %>% 
  select(1:13)
  
# Convert dataframe from wide to long format
tidyweather <- weather_clean %>% 
  pivot_longer(cols = 2:13,
               names_to = "month",
               values_to = "delta") %>% 
  rename("year" = "Year")
  
```

## Plotting Information

Let us plot the data using a time-series scatter plot, and add a
trendline. To do that, we first need to create a new variable called
`date` in order to ensure that the `delta` values are plot
chronologically.

```{r scatter_plot}

# Add date column to dataframe
tidyweather <- tidyweather %>%
  mutate(date = ymd(paste(as.character(year), month, "1")),
         month = month(date, label = TRUE),
         year = year(date))
tidyweather

# Plot scatterplot of delta over time with best fit line
ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point(alpha = 0.3)+
  geom_smooth(color="red") +
  theme_bw() +
  labs (
    title = "Weather Anomalies",
    x = "Date",
    y = "Delta")

```

> Is the effect of increasing temperature more pronounced in some months?

The graph below shows that in the period between October and March, we can observe a slightly steeper increase in temperatures than for the remaining months. Especially February and March clearly exhibit the highest delta in recent years. We can hypothesize that the more significant increase in temperature is linked to snowfall. We would expect snowfall in many countries of the Northern Hemisphere during that time, which might have a cooling effect on temperatures. As temperatures increase over time, less snowfall is expected and temperatures rise more drastically compared to the base period. in which snowfall was still more common. Of course, such a claim would have to be investigated further through hypothesis testing.

Use `facet_wrap()` to produce a separate scatter plot for each month,
again with a smoothing line. Your chart should human-readable labels;
that is, each month should be labeled "Jan", "Feb", "Mar" (full or
abbreviated month names are fine), not `1`, `2`, `3`.

```{r facet_wrap, echo=FALSE}

# Create faceted scatterplot for delta over years by month
tidyweather %>% 
  ggplot(aes(x = year, y = delta)) +
    geom_point() +
    geom_smooth() +
    facet_wrap(~ month) +
    theme_bw() +
    labs(title = "Weather deviations have seen a significant increase in all months since 1880", 
           subtitle = "Weather deviations from 1880 to 2022 in the Northern Hemisphere",
           x = "Year",
           y = "Delta")

```

We remove data before 1881 and before using `filter`. Then, we use the
`mutate` function to create a new variable `interval` which contains
information on which period each observation belongs to. We can assign
the different periods using `case_when()`.

```{r intervals}

comparison <- tidyweather %>% 
  filter(year >= 1881) %>% # remove years prior to 1881
  # Create new variable 'interval', and assign values based on criteria below:
  mutate(interval = case_when(
    year %in% c(1881:1920) ~ "1881-1920",
    year %in% c(1921:1950) ~ "1921-1950",
    year %in% c(1951:1980) ~ "1951-1980",
    year %in% c(1981:2010) ~ "1981-2010",
    TRUE ~ "2011-present"
  ))

```

Now that we have the `interval` variable, we can create a density plot
to study the distribution of monthly deviations (`delta`), grouped by
the different time periods we are interested in. Set `fill` to
`interval` to group and colour the data by different time periods.

```{r density_plot}

# Create density plot separated by all time periods
comparison %>% 
  ggplot(aes(x = delta, fill = interval)) +
    geom_density(alpha = 0.3) + # Change alpha to make lines more visible
    theme_bw() +
    labs(title = "The average weather deviation is increasing from period to period", 
           subtitle = "Distribution of weather deviations by period",
           x = "Delta",
           y = "Distribution")

```

So far, we have been working with monthly anomalies. However, we might
be interested in average annual anomalies. We can do this by using
`group_by()` and `summarise()`, followed by a scatter plot to display
the result.

```{r averaging}

# Creating yearly averages
average_annual_anomaly <- tidyweather %>% 
  group_by(year) %>%   # Grouping data by Year
  # Creating summaries for mean delta 
  # Use `na.rm=TRUE` to eliminate NA (not available) values 
  summarise(anl_mean_delta = mean(delta), na.rm = TRUE) 

# Create a scatterplot of the annual mean delta over years
average_annual_anomaly %>% 
  ggplot(aes(x = year, y = anl_mean_delta)) +
  geom_point() +
  geom_smooth() +
  theme_bw() +
  labs(title = "Climate in the Northern Hemisphere has been increasing exponentially since 1960", 
           subtitle = "Development of annual weather deviations from 1881 to 2022",
           x = "Years",
           y = "Average Annual Delta")

```

## Confidence Interval for `delta`

[NASA points out on their
website](https://earthobservatory.nasa.gov/world-of-change/decadaltemp.php)
that

> A one-degree global change is significant because it takes a vast
> amount of heat to warm all the oceans, atmosphere, and land by that
> much. In the past, a one- to two-degree drop was all it took to plunge
> the Earth into the Little Ice Age.

We construct a confidence interval for the average annual
delta since 2011, both using a formula and using a bootstrap simulation
with the `infer` package. Recall that the dataframe `comparison` has
already grouped temperature anomalies according to time intervals; we
are only interested in what is happening between 2011-present.

```{r, calculate_CI_using_formula}

# Create confidence interval per year since 2011 with the formula
formula_ci <- comparison %>%
  # Choose the interval 2011-present
  filter(interval == "2011-present") %>% 
  # Remove NA values in 2022 to compute summary statistics and CI
  drop_na() %>% 
  # Calculate mean, SD, count, SE, lower/upper 95% CI
  summarise(mean_delta = mean(delta),
            median_delta = median(delta),
            sd_delta = sd(delta),
            count = n(),
            t_critical = qt(0.975, count-1),
            se_delta = sd_delta/sqrt(count),
            margin_of_error = t_critical * se_delta,
            delta_low = mean_delta - margin_of_error,
            delta_high = mean_delta + margin_of_error,)

# Print out formula_ci
formula_ci

```

```{r calculate_CI_using_infer}

# Set seed for reproducibility
set.seed(1234)

delta_2011_bootstrap <- comparison %>% 
  # Choose the interval 2011-present
  filter(interval == "2011-present") %>%
  # Specify the variable of interest
  specify(response = delta) %>%
  # Generate bootstrap samples
  generate(reps = 1000, type = "bootstrap") %>%
  # Find the mean of each sample
  calculate(stat = "mean")

# Create the confidence interval of bootstrapped samples
bootstrap_ci <- delta_2011_bootstrap %>% 
  get_confidence_interval(level = 0.95, type = "percentile")

# Print out bootstrap_ci
bootstrap_ci
```

> What is the data showing us?

Looking at the 95% confidence interval constructed with either formulas or the infer package, we observe identical values for the interval min and max values. To create the confidence interval with the formula approach, we started by filtering for the time period of interest and removed the months for which we do not yet have data in 2022. We then proceeded to calculate the summary statistics mean, median, standard deviation and count to set the basis for our confidence interval. As mean and median are very similar, we can assume a normal distribution of values. Given the sample size of 139, we have a t_critical value rather close to 1.96 (the t_critical for the bootleg approach will be even closer, given the sample size of 1000). We used the `qt()`function with a confidence level of 95%, deducting 1 from the count to represent the degree of freedom. As we want to make assertions about the population with the confidence interval, we calculate the standard error and margin of error. By deducting and adding the margin of error to the mean in turn, we are able to develop both the min and max margin of the two-sided confidence interval. The resulting min and max values enable us to claim with a 95% confidence that the temperature in the Northern Hemisphere was between 1.02 and 1.11 degrees Celsius higher since 2011 than the baseline temperature (1951 to 1980). This implies that even at the minimum value of our confidence interval, we would surpass the critical threshold of 1 degree change as defined by NASA. Perhaps climate change is not a [hoax](https://www.forbes.com/sites/markjoyella/2022/03/21/on-fox-donald-trump-calls-climate-change-a-hoax-in-the-1920s-they-were-talking-about-global-freezing/) after all.